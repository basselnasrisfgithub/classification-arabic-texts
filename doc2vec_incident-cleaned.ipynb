{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tags                                               post\n",
      "1    11  عند التاسعة من مساء يوم 29/9/2018 وفي محلة الط...\n",
      "2     4  اقدم المدعو وليد عدنان مرعي على شتم و اهانة ال...\n",
      "3    12  أقدم المدعو ناصر رضا على اصدار شك لصالح حسن اح...\n",
      "4     1  الساعة 7.30 من تاريخه وفي محلة فردان شارع كرام...\n"
     ]
    }
   ],
   "source": [
    "#readind and preparing data ( text , label )\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_excel('incident13type.xlsx', header=None) ## read le file\n",
    "df.columns=['tags','post']\n",
    "print(df[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('arabic'))\n",
    "from bs4 import BeautifulSoup\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9ا-ي #+_]')\n",
    "STOPWORDS = set(stopwords.words('arabic'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['post'] = df['post'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التاسعة مسا يوم           وفي محلة الطريق الجديدة ارض جلول اوقفت دورية مكتب معلومات بيروت الرابعة المدعو خالد عسكر الاشتباه بترويج المخدرات عثرت بحوزته كمية المخدرات الشكل التالي    حبة نوع بنزكسول       حبة نوع بنزكسول موضبين داخل ثلاثة اكياس صغيرة   حبات ونصف نوع ترامال   حبة نوع المخدرة للاعصاب   حبة مجهولة الصنف لون اصفر داخل حنجور بالاضافة الى    كارت نوع تيليكارت مغلفين\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التاسعة مسا يوم           وفي محلة الطريق الجديدة ارض جلول اوقفت دورية مكتب معلومات بيروت الرابعة المدعو خالد عسكر الاشتباه بترويج المخدرات عثرت بحوزته كمية المخدرات الشكل التالي    حبة نوع بنزكسول       حبة نوع بنزكسول موضبين داخل ثلاثة اكياس صغيرة   حبات ونصف نوع ترامال   حبة نوع المخدرة للاعصاب   حبة مجهولة الصنف لون اصفر داخل حنجور بالاضافة الى    كارت نوع تيليكارت مغلفين\n"
     ]
    }
   ],
   "source": [
    "print(df['post'][1])\n",
    "table=str.maketrans(\"+=_-<>~/!*%?:;,)&1234567890@$qwertyuioplkjhgfdsamnbvcxz(QWERTYUIOPLKJHGFDSAZXCVBNM\",82*\" \")\n",
    "for i in range(len(df['post'])):\n",
    "    df['post'][i] = df['post'][i].translate(table)\n",
    "print(df['post'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['الساعة', 'تاريخه', 'اصطدمت', 'الدراجة', 'النارية', 'نوع', 'لين', 'هاي', 'لون', 'موف', 'صنع', 'رقم', 'م', 'بقيادة', 'المدعو', 'محمد', 'احمد', 'عواضة', 'والدته', 'حنان', 'تولد', 'لبناني', 'بفان', 'نوع', 'مازدا', 'صنع', 'لون', 'ابيض', 'رقم', 'م', 'بقيادة', 'علي', 'عباس', 'زعيتر', 'والدته', 'نور', 'تولد', 'لبناني', 'نتج', 'الحادث', 'اصابة', 'الاول', 'بكسور', 'الوجه', 'نقل', 'الى', 'مستشفى', 'الجامعة', 'الاميريكية', 'للمعالجة', 'نظم', 'محضر', 'رقم'], tags=['Train_0']),\n",
       " TaggedDocument(words=['سلمتنا', 'دورية', 'فصيلتنا', 'المدعو', 'خالد', 'عيسى', 'سوري', 'بجرم', 'دخول', 'البلاد', 'خلسة', 'وذلك', 'توقيفه', 'محلة', 'الروشة', 'الساعة'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1193082.06it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2378501.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1192939.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2391585.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2386448.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2386448.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1190808.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2387018.56it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1192797.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1199810.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2349440.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2386448.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2379633.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2381901.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2383604.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2386448.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1202986.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1193224.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1192655.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2370607.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2389300.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1192655.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2384741.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2354419.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1200098.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1194079.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2352757.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2392730.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2384741.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1184882.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1186006.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7751396648044693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.76      0.70        83\n",
      "           2       0.45      0.48      0.47        31\n",
      "           3       0.95      0.95      0.95       120\n",
      "           4       0.30      0.33      0.32        21\n",
      "           5       0.40      0.53      0.46        15\n",
      "           6       0.27      0.20      0.23        20\n",
      "           7       0.88      0.81      0.84       150\n",
      "           8       0.93      0.87      0.90        15\n",
      "           9       0.89      0.87      0.88        62\n",
      "          10       0.53      0.51      0.52        39\n",
      "          11       0.98      0.95      0.97        63\n",
      "          12       0.89      0.96      0.92        25\n",
      "          13       0.75      0.72      0.74        72\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       716\n",
      "   macro avg       0.68      0.69      0.68       716\n",
      "weighted avg       0.78      0.78      0.78       716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
