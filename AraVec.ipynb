{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ابوتريكه 0.9565805792808533\n",
      "حازم_امام 0.864891767501831\n",
      "وائل_جمعه 0.8543370366096497\n",
      "تريكه 0.8521531820297241\n",
      "حسام_غالي 0.846001148223877\n",
      "عماد_متعب 0.8435681462287903\n",
      "حسن_شحاته 0.8425122499465942\n",
      "عمرو_زكي 0.8408412337303162\n",
      "حسام_حسن 0.8271308541297913\n",
      "رمضان_صبحي 0.8270741701126099\n",
      "راشد_الماجد 0.7094648480415344\n",
      "ماجد_المهندس 0.6979794502258301\n",
      "عبدالله_رويشد 0.6942605376243591\n",
      "عبدالله_الرويشد 0.6927955746650696\n",
      "خالد_عبدالرحمن 0.6894348859786987\n",
      "رابح_صقر 0.684174120426178\n",
      "عبدالمجيد_عبدالله 0.684122622013092\n",
      "محمد_عبده 0.6824554204940796\n",
      "نبيل_شعيل 0.6798837184906006\n",
      "زايد_الصالح 0.6735830903053284\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "# =========================\n",
    "# ==== Helper Methods =====\n",
    "\n",
    "# Clean/Normalize Arabic Text\n",
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_vec(n_model,dim, token):\n",
    "    vec = np.zeros(dim)\n",
    "    is_vec = False\n",
    "    if token not in n_model.wv:\n",
    "        _count = 0\n",
    "        is_vec = True\n",
    "        for w in token.split(\"_\"):\n",
    "            if w in n_model.wv:\n",
    "                _count += 1\n",
    "                vec += n_model.wv[w]\n",
    "        if _count > 0:\n",
    "            vec = vec / _count\n",
    "    else:\n",
    "        vec = n_model.wv[token]\n",
    "    return vec\n",
    "\n",
    "def calc_vec(pos_tokens, neg_tokens, n_model, dim):\n",
    "    vec = np.zeros(dim)\n",
    "    for p in pos_tokens:\n",
    "        vec += get_vec(n_model,dim,p)\n",
    "    for n in neg_tokens:\n",
    "        vec -= get_vec(n_model,dim,n)\n",
    "    \n",
    "    return vec   \n",
    "\n",
    "## -- Retrieve all ngrams for a text in between a specific range\n",
    "def get_all_ngrams(text, nrange=3):\n",
    "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
    "    ngs = []\n",
    "    for n in range(2,nrange+1):\n",
    "        ngs += [ng for ng in ngrams(tokens, n)]\n",
    "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
    "\n",
    "## -- Retrieve all ngrams for a text in a specific n\n",
    "def get_ngrams(text, n=2):\n",
    "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
    "    ngs = [ng for ng in ngrams(tokens, n)]\n",
    "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
    "\n",
    "## -- filter the existed tokens in a specific model\n",
    "def get_existed_tokens(tokens, n_model):\n",
    "    return [tok for tok in tokens if tok in n_model.wv ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================   \n",
    "# ====== N-Grams Models ======\n",
    "\n",
    "t_model = gensim.models.Word2Vec.load('models/full_grams_cbow_100_twitter.mdl')\n",
    "\n",
    "# python 3.X\n",
    "token = clean_str(u'ابو تريكه').replace(\" \", \"_\")\n",
    "# python 2.7\n",
    "# token = clean_str(u'ابو تريكه'.decode('utf8', errors='ignore')).replace(\" \", \"_\")\n",
    "\n",
    "if token in t_model.wv:\n",
    "    most_similar = t_model.wv.most_similar( token, topn=10 )\n",
    "    for term, score in most_similar:\n",
    "        term = clean_str(term).replace(\" \", \"_\")\n",
    "        if term != token:\n",
    "            print(term, score)\n",
    "\n",
    "# تريكه 0.752911388874054\n",
    "# حسام_غالي 0.7516342401504517\n",
    "# وائل_جمعه 0.7244222164154053\n",
    "# وليد_سليمان 0.7177559733390808\n",
    "# ...\n",
    "\n",
    "# =========================================\n",
    "# == Get the most similar tokens to a compound query\n",
    "# most similar to \n",
    "# عمرو دياب + الخليج - مصر\n",
    "\n",
    "pos_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['عمرو دياب', 'الخليج'] if t.strip() != \"\"]\n",
    "neg_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['مصر'] if t.strip() != \"\"]\n",
    "\n",
    "vec = calc_vec(pos_tokens=pos_tokens, neg_tokens=neg_tokens, n_model=t_model, dim=t_model.vector_size)\n",
    "\n",
    "most_sims = t_model.wv.similar_by_vector(vec, topn=10)\n",
    "for term, score in most_sims:\n",
    "    if term not in pos_tokens+neg_tokens:\n",
    "        print(term, score)\n",
    "\n",
    "# راشد_الماجد 0.7094649076461792\n",
    "# ماجد_المهندس 0.6979793906211853\n",
    "# عبدالله_رويشد 0.6942606568336487\n",
    "# ...\n",
    "\n",
    "# ====================\n",
    "# ====================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================== \n",
    "# ====== Uni-Grams Models ======\n",
    "\n",
    "##t_model = gensim.models.Word2Vec.load('models/full_uni_cbow_100_twitter.mdl')\n",
    "\n",
    "# python 3.X\n",
    "##token = clean_str(u'تونس')\n",
    "# python 2.7\n",
    "# token = clean_str('تونس'.decode('utf8', errors='ignore'))\n",
    "\n",
    "##most_similar = t_model.wv.most_similar( token, topn=10 )\n",
    "##for term, score in most_similar:\n",
    "####    print(term, score)\n",
    "\n",
    "# ليبيا 0.8864325284957886\n",
    "# الجزائر 0.8783721327781677\n",
    "# السودان 0.8573237061500549\n",
    "# مصر 0.8277812600135803\n",
    "# ...\n",
    "\n",
    "\n",
    "\n",
    "# get a word vector\n",
    "word_vector = t_model.wv[ token ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وسرقه 0.8054364919662476\n",
      "مصادره 0.745844304561615\n",
      "لسرقه 0.732149600982666\n",
      "اتلاف 0.7281239032745361\n",
      "حيازه 0.7137411832809448\n",
      "تهريب 0.71235191822052\n",
      "اختلاس 0.7055280208587646\n",
      "سرقات 0.7028773427009583\n",
      "بيع 0.6991260051727295\n",
      "خطف 0.6953885555267334\n",
      "قصيميه 0.613539457321167\n",
      "عتيبيه 0.5760815143585205\n",
      "قرويه 0.5639134049415588\n",
      "حجازيه 0.549694299697876\n",
      "طخمه 0.5465852618217468\n",
      "مصوره 0.5462727546691895\n",
      "بكستانيه 0.5425755977630615\n",
      "رسامه 0.5425392389297485\n",
      "قصمنجيه 0.540238082408905\n",
      "غامديه 0.5364717245101929\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "# =========================\n",
    "# ==== Helper Methods =====\n",
    "\n",
    "# Clean/Normalize Arabic Text\n",
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_vec(n_model,dim, token):\n",
    "    vec = np.zeros(dim)\n",
    "    is_vec = False\n",
    "    if token not in n_model.wv:\n",
    "        _count = 0\n",
    "        is_vec = True\n",
    "        for w in token.split(\"_\"):\n",
    "            if w in n_model.wv:\n",
    "                _count += 1\n",
    "                vec += n_model.wv[w]\n",
    "        if _count > 0:\n",
    "            vec = vec / _count\n",
    "    else:\n",
    "        vec = n_model.wv[token]\n",
    "    return vec\n",
    "\n",
    "def calc_vec(pos_tokens, neg_tokens, n_model, dim):\n",
    "    vec = np.zeros(dim)\n",
    "    for p in pos_tokens:\n",
    "        vec += get_vec(n_model,dim,p)\n",
    "    for n in neg_tokens:\n",
    "        vec -= get_vec(n_model,dim,n)\n",
    "    \n",
    "    return vec   \n",
    "\n",
    "## -- Retrieve all ngrams for a text in between a specific range\n",
    "def get_all_ngrams(text, nrange=3):\n",
    "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
    "    ngs = []\n",
    "    for n in range(2,nrange+1):\n",
    "        ngs += [ng for ng in ngrams(tokens, n)]\n",
    "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
    "\n",
    "## -- Retrieve all ngrams for a text in a specific n\n",
    "def get_ngrams(text, n=2):\n",
    "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
    "    ngs = [ng for ng in ngrams(tokens, n)]\n",
    "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
    "\n",
    "## -- filter the existed tokens in a specific model\n",
    "def get_existed_tokens(tokens, n_model):\n",
    "    return [tok for tok in tokens if tok in n_model.wv ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================   \n",
    "# ====== N-Grams Models ======\n",
    "\n",
    "t_model = gensim.models.Word2Vec.load('models/full_grams_cbow_100_twitter.mdl')\n",
    "\n",
    "# python 3.X\n",
    "token = clean_str(u'سرقة').replace(\" \", \"_\")\n",
    "# python 2.7\n",
    "# token = clean_str(u'ابو تريكه'.decode('utf8', errors='ignore')).replace(\" \", \"_\")\n",
    "\n",
    "if token in t_model.wv:\n",
    "    most_similar = t_model.wv.most_similar( token, topn=10 )\n",
    "    for term, score in most_similar:\n",
    "        term = clean_str(term).replace(\" \", \"_\")\n",
    "        if term != token:\n",
    "            print(term, score)\n",
    "\n",
    "# تريكه 0.752911388874054\n",
    "# حسام_غالي 0.7516342401504517\n",
    "# وائل_جمعه 0.7244222164154053\n",
    "# وليد_سليمان 0.7177559733390808\n",
    "# ...\n",
    "\n",
    "# =========================================\n",
    "# == Get the most similar tokens to a compound query\n",
    "# most similar to \n",
    "# عمرو دياب + الخليج - مصر\n",
    "\n",
    "pos_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['انطوان سماحة', 'سعودية'] if t.strip() != \"\"]\n",
    "neg_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['لبنان'] if t.strip() != \"\"]\n",
    "\n",
    "vec = calc_vec(pos_tokens=pos_tokens, neg_tokens=neg_tokens, n_model=t_model, dim=t_model.vector_size)\n",
    "\n",
    "most_sims = t_model.wv.similar_by_vector(vec, topn=10)\n",
    "for term, score in most_sims:\n",
    "    if term not in pos_tokens+neg_tokens:\n",
    "        print(term, score)\n",
    "\n",
    "# راشد_الماجد 0.7094649076461792\n",
    "# ماجد_المهندس 0.6979793906211853\n",
    "# عبدالله_رويشد 0.6942606568336487\n",
    "# ...\n",
    "\n",
    "# ====================\n",
    "# ====================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================== \n",
    "# ====== Uni-Grams Models ======\n",
    "\n",
    "##t_model = gensim.models.Word2Vec.load('models/full_uni_cbow_100_twitter.mdl')\n",
    "\n",
    "# python 3.X\n",
    "##token = clean_str(u'تونس')\n",
    "# python 2.7\n",
    "# token = clean_str('تونس'.decode('utf8', errors='ignore'))\n",
    "\n",
    "##most_similar = t_model.wv.most_similar( token, topn=10 )\n",
    "##for term, score in most_similar:\n",
    "####    print(term, score)\n",
    "\n",
    "# ليبيا 0.8864325284957886\n",
    "# الجزائر 0.8783721327781677\n",
    "# السودان 0.8573237061500549\n",
    "# مصر 0.8277812600135803\n",
    "# ...\n",
    "\n",
    "\n",
    "\n",
    "# get a word vector\n",
    "word_vector = t_model.wv[ token ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12898168 -0.12776525 -2.3782592   1.5905364   2.1040397   0.6249796\n",
      "  1.6603498   0.26098573  0.19134237 -0.38898647  0.06065198 -0.30491403\n",
      "  3.223809   -0.5292832   0.18292628 -3.4605308  -3.1353152  -0.39001396\n",
      "  0.7233246   0.93164885 -1.71681     1.3676271  -2.7011962   2.3073795\n",
      " -0.19108006  0.575296   -0.71562177 -2.1097858   0.9799234   0.296826\n",
      "  0.6394561  -0.4804854   0.41199136 -0.73732334 -3.0052764   3.4832702\n",
      "  1.2604014   1.2374989  -0.9858779   0.912821   -0.9510982   1.1159695\n",
      "  1.2028342  -2.1836925  -0.5642931   1.4381927   0.9122004  -1.4751711\n",
      "  2.0118773  -2.2427366  -1.0178751   3.3120286  -3.4177668   1.0405232\n",
      " -2.3204424  -2.8205185   0.8214644  -3.101375   -0.8910767  -0.835392\n",
      "  1.2876183   0.38810432  1.2272987  -1.6125118   0.22605917 -1.6748503\n",
      " -1.4342185   0.519484   -0.03773731 -0.12334368 -0.9083439   1.500213\n",
      " -1.8346211  -1.3816308  -1.3337007   0.64141244 -0.7631837  -0.5235907\n",
      " -0.9442183  -1.9284645  -0.7089352   1.2635968   1.4791634   1.9858744\n",
      " -2.6170046  -3.4504976   2.8139973  -5.1915474   0.59085435  3.895668\n",
      "  2.683087    0.9023208   0.35032722  1.0560318  -0.07811852 -2.2468443\n",
      " -0.35539728 -3.057128    2.758906   -2.2831123 ]\n"
     ]
    }
   ],
   "source": [
    "print(word_vector)\n",
    "#index2word_set = set(word2vec_vectors.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '%', ':', 'من', '#', 'رابطويب', 'في', 'الله', ')', '(', 'و', 'علي', '!', 'لا', 'ان', 'ما', ']', '؟', 'اللهم', 'الا', 'كل', 'انا', 'يا', ',', 'اللي', 'ولا', 'الي', 'لا_اله', 'ي', 'عن', '…', 'هو', 'بس', 'انت', 'ب', 'لي', 'مع', 'م', 'ل', 'هذا', 'اللهم_اني', 'لو', 'ف', 'هه', 'كان', 'لك', 'اذا', 'الله_اكبر', 'او', '”', 'بعد', 'والله', 'ه', 'ع', 'الا_الله', 'الناس', 'ا', 'سبحان', 'حتي', 'فيه', 'له', 'ال', 'يوم', 'اي', '*', 'قلبي', 'الذي', 'ك', 'اني', 'شي', 'انك', '?', 'قال', 'احد', 'وانا', 'انه', 'هي', 'قبل', 'يارب', 'استغفر', 'اليوم', 'الحمدلله', 'مش', 'ت', ';', '“', 'ربي', 'يعني', 'غير', 'لما', 'شاهد', 'وكل', 'خير', 'عليه', 'الدنيا', 'ن', 'رب', '♡', '=', 'غرد_بذكر']\n",
      "[-2.03215146e+00  2.36172247e+00 -1.07531315e-02 -1.33089781e+00\n",
      "  4.09337610e-01  1.67818296e+00  4.49576092e+00  1.24167097e+00\n",
      "  2.07176134e-01 -5.34855890e+00 -1.27874267e+00  2.15164528e-05\n",
      "  2.37422538e+00 -3.59334707e-01 -8.83399129e-01 -2.79245257e+00\n",
      " -2.01776457e+00 -1.24322498e+00  1.48167551e+00 -2.01816034e+00\n",
      " -3.49973655e+00 -2.23054385e+00 -3.79678607e+00  3.66574097e+00\n",
      " -1.66383636e+00 -3.00195765e+00 -2.08964944e+00 -2.87615299e+00\n",
      "  9.03220475e-01 -2.40501308e+00 -2.18862367e+00 -3.22108674e+00\n",
      "  1.27333149e-01 -1.27445066e+00  2.00740290e+00 -1.39079916e+00\n",
      "  2.52500534e+00  1.30421650e+00  3.25385237e+00 -5.10480225e-01\n",
      "  2.82267714e+00  5.66742516e+00  3.62578702e+00  7.51243830e-02\n",
      "  3.67458820e+00  2.54219055e+00  2.77229285e+00 -3.27410531e+00\n",
      "  1.27093911e+00  4.99585062e-01 -1.16014385e+00 -5.17638505e-01\n",
      " -2.42966819e+00  2.32615566e+00  6.90570354e-01 -1.09240664e-02\n",
      " -7.52090096e-01 -3.85866618e+00 -3.58540863e-02 -4.90011263e+00\n",
      "  4.50870132e+00 -8.24709356e-01  2.63521361e+00  3.41764569e+00\n",
      " -2.74463415e+00 -1.04336810e+00  3.28004932e+00  1.93088889e+00\n",
      " -1.90355539e+00  9.30068433e-01 -8.85993838e-01  2.41974497e+00\n",
      "  1.30995095e+00 -2.50449610e+00 -2.59586334e-01  4.99726725e+00\n",
      " -1.81738079e+00 -7.83044457e-01  1.39107478e+00  6.99124694e-01\n",
      " -3.62874722e+00 -1.03379369e+00  1.64517689e+00  2.94912481e+00\n",
      "  1.60912108e+00  1.49734759e+00  5.39454746e+00 -2.83148080e-01\n",
      "  3.59865451e+00  1.51458263e+00  3.38738799e+00 -3.18688393e+00\n",
      " -8.59866202e-01  8.88811648e-01 -1.97571945e+00 -5.00384903e+00\n",
      " -2.61505890e+00  2.77311862e-01  5.53445637e-01 -1.55879593e+00]\n"
     ]
    }
   ],
   "source": [
    "#collection1=collection[2:9]\n",
    "#for i in range(len(collection1)):\n",
    " #   word_vector=t_model.wv(collection1[i].split())\n",
    "collection1='محضر تحقيق بالشكوى المقدمة من المدعوة رانيا المصري صاحبة مؤسسة بوكالة المحامية نسرين حرب ضد المدعوة هدى فرحات بجرم قدح وذم ومخالفة قرار إداري '\n",
    "#for i in range(len(collection1)):\n",
    "index2word_set = t_model.wv.index2word\n",
    "\n",
    "print(index2word_set[0:100])\n",
    "print(t_model.wv['من'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
