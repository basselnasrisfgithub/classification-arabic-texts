{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "1      0  محضر تحقيق بادعاء المدعوة اسما هدى انيس قساطلي...\n",
      "2      1  محضر تحقيق بحق المدعو ماهر صالح الحرامي سوري ل...\n",
      "3      0  محضر بتنفيذ خلاصة حكم بحق المدعو محمد سعد الدي...\n",
      "4      0  محضر بتنفيذ قرار جزائي بحق المدعو محمد سعد الد...\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#readind and preparing data ( text , label )\n",
    "data=pd.read_excel('ML1b.xlsx', header=None) ## read le file\n",
    "data.columns=['label','text']\n",
    "print(data[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtb=data['text']\n",
    "#X_train=Xt[0:5000]\n",
    "#print(X_train[1])\n",
    "Ytb=data['label']\n",
    "#Y_train=Yt[0:5000]\n",
    "##print(Y_train[1])\n",
    "#X_test=Xt[5001:]\n",
    "#Y_test=Yt[5001:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Arabic Transliteration based on Buckwalter\n",
    "# dictionary source is buckwalter2unicode.py http://www.redhat.com/archives/fedora-extras-commits/2007-June/msg03617.html \n",
    "\n",
    "buck2uni = {\"'\": u\"\\u0621\", # hamza-on-the-line\n",
    "            \"|\": u\"\\u0622\", # madda\n",
    "            \">\": u\"\\u0623\", # hamza-on-'alif\n",
    "            \"&\": u\"\\u0624\", # hamza-on-waaw\n",
    "            \"<\": u\"\\u0625\", # hamza-under-'alif\n",
    "            \"}\": u\"\\u0626\", # hamza-on-yaa'\n",
    "            \"A\": u\"\\u0627\", # bare 'alif\n",
    "            \"b\": u\"\\u0628\", # baa'\n",
    "            \"p\": u\"\\u0629\", # taa' marbuuTa\n",
    "            \"t\": u\"\\u062A\", # taa'\n",
    "            \"v\": u\"\\u062B\", # thaa'\n",
    "            \"j\": u\"\\u062C\", # jiim\n",
    "            \"H\": u\"\\u062D\", # Haa'\n",
    "            \"x\": u\"\\u062E\", # khaa'\n",
    "            \"d\": u\"\\u062F\", # daal\n",
    "            \"*\": u\"\\u0630\", # dhaal\n",
    "            \"r\": u\"\\u0631\", # raa'\n",
    "            \"z\": u\"\\u0632\", # zaay\n",
    "            \"s\": u\"\\u0633\", # siin\n",
    "            \"$\": u\"\\u0634\", # shiin\n",
    "            \"S\": u\"\\u0635\", # Saad\n",
    "            \"D\": u\"\\u0636\", # Daad\n",
    "            \"T\": u\"\\u0637\", # Taa'\n",
    "            \"Z\": u\"\\u0638\", # Zaa' (DHaa')\n",
    "            \"E\": u\"\\u0639\", # cayn\n",
    "            \"g\": u\"\\u063A\", # ghayn\n",
    "            \"_\": u\"\\u0640\", # taTwiil\n",
    "            \"f\": u\"\\u0641\", # faa'\n",
    "            \"q\": u\"\\u0642\", # qaaf\n",
    "            \"k\": u\"\\u0643\", # kaaf\n",
    "            \"l\": u\"\\u0644\", # laam\n",
    "            \"m\": u\"\\u0645\", # miim\n",
    "            \"n\": u\"\\u0646\", # nuun\n",
    "            \"h\": u\"\\u0647\", # haa'\n",
    "            \"w\": u\"\\u0648\", # waaw\n",
    "            \"Y\": u\"\\u0649\", # 'alif maqSuura\n",
    "            \"y\": u\"\\u064A\", # yaa'\n",
    "            \"F\": u\"\\u064B\", # fatHatayn\n",
    "            \"N\": u\"\\u064C\", # Dammatayn\n",
    "            \"K\": u\"\\u064D\", # kasratayn\n",
    "            \"a\": u\"\\u064E\", # fatHa\n",
    "            \"u\": u\"\\u064F\", # Damma\n",
    "            \"i\": u\"\\u0650\", # kasra\n",
    "            \"~\": u\"\\u0651\", # shaddah\n",
    "            \"o\": u\"\\u0652\", # sukuun\n",
    "            \"`\": u\"\\u0670\", # dagger 'alif\n",
    "            \"{\": u\"\\u0671\", # waSla\n",
    "}\n",
    "def transString(string, reverse=0):\n",
    "    '''Given a Unicode string, transliterate into Buckwalter. To go from\n",
    "    Buckwalter back to Unicode, set reverse=1'''\n",
    "\n",
    "    for k, v in buck2uni.items():\n",
    "      if not reverse:\n",
    "            string = string.replace(v, k)\n",
    "      else:\n",
    "            string = string.replace(k, v)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-181ba7afa2f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mXb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-52c4db1cad91>\u001b[0m in \u001b[0;36mtransString\u001b[1;34m(string, reverse)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbuck2uni\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "for i in range(len(Xtb)):\n",
    "    Xb[i]=transString(Xtb[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mHDr tHqyq bAdEA' fwAz mHmd Abw Emr/ lbnAny Dd mjhwlyn bjrm srqp bwAsTp Alksr wAlxlE. \n",
      "mHDr tHqyq bAdEA' fwAz mHmd Abw Emr/ lbnAny Dd mjhwlyn bjrm srqp bwAsTp Alksr wAlxlE. \n"
     ]
    }
   ],
   "source": [
    "len(Xt)\n",
    "print(data['text'][8])\n",
    "print(Xt[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2262e1e26e18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mSW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#ar_stemmer = stemmer(\"arabic\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#ar_stemmer.stemWord(u\"فسميتموها\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import stemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "SW=set(stopwords.words('english'))\n",
    "#ar_stemmer = stemmer(\"arabic\")\n",
    "#ar_stemmer.stemWord(u\"فسميتموها\")\n",
    "def prep(doc):\n",
    "    words=str(doc).strip().split()###  strip pr eliminer les white spaces, lower por transformer en miniscules\n",
    "    #stemmer=SnowballStemmer(\"arabic\")\n",
    "    #ar_stemmer = stemmer(\"arabic\")\n",
    "    #words=ar_stemmer.stem(w.decode('UTF-8'))\n",
    "    words=[stemmer.stem(w) for w in words  if not w in SW]## transformer en stem without les stopwords\n",
    "    return ' '.join(words)## met des spaces entre les mots\n",
    "xt=Xt.apply(prep)#### select le colomn text selement et la preper(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(doc):\n",
    "    words=str(doc).strip().split()\n",
    "    return ''.join(words)\n",
    "XT=Xt.apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model1=TfidfVectorizer()\n",
    "tfidf_matrix=model1.fit_transform(XT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels=data['label']\n",
    "train,test,ltrain,ltest =\\\n",
    "train_test_split(tfidf_matrix, labels,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.10071537\n",
      "Iteration 2, loss = 1.41568510\n",
      "Iteration 3, loss = 0.87649561\n",
      "Iteration 4, loss = 0.54118995\n",
      "Iteration 5, loss = 0.36983646\n",
      "Iteration 6, loss = 0.27539956\n",
      "Iteration 7, loss = 0.19484270\n",
      "Iteration 8, loss = 0.16269285\n",
      "Iteration 9, loss = 0.16689769\n",
      "Iteration 10, loss = 0.15249663\n",
      "Iteration 11, loss = 0.14887979\n",
      "Iteration 12, loss = 0.13974364\n",
      "Iteration 13, loss = 0.13645701\n",
      "Iteration 14, loss = 0.12231139\n",
      "Iteration 15, loss = 0.12030324\n",
      "Iteration 16, loss = 0.13419733\n",
      "Iteration 17, loss = 0.14809884\n",
      "Iteration 18, loss = 0.21203003\n",
      "Iteration 19, loss = 0.26580160\n",
      "Iteration 20, loss = 0.27561326\n",
      "Iteration 21, loss = 0.31295914\n",
      "Iteration 22, loss = 0.27506503\n",
      "Iteration 23, loss = 0.25585115\n",
      "Iteration 24, loss = 0.20318597\n",
      "Iteration 25, loss = 0.18493851\n",
      "Iteration 26, loss = 0.18097808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(50,30,), max_iter=120, solver='adam',verbose =10 , tol=1e-4, random_state=2, learning_rate_init=0.1)\n",
    "model=mlp.fit(train.todense(),ltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[ 25   0   1   0   0   2  11  14   3   4   0   0   5]\n",
      " [ 10   1   1   1   2   9   0   0  15   9   0   0  10]\n",
      " [ 10   2  33   0   0   3   0   0   9  12   3   0   5]\n",
      " [  9   1   2  15   1  11   0   0  10  14   0   0  10]\n",
      " [  7   2   0   1  12  26   0   0  14   9   0   6  17]\n",
      " [  9   1   0   8   4  17   0   2  20  15   2   1  13]\n",
      " [ 45   0   1   0   0   0  23  32   0   1   0   0   0]\n",
      " [ 60   0   0   0   0   0   0  47   0   0   0   0   0]\n",
      " [ 13   0   2   0   0   3   0   1  65  11   0   0  12]\n",
      " [ 19   0   2   7   0   2   0   2  23  42   2   0  24]\n",
      " [125   0   1   3   0   1   0  10   0   4  15   0   1]\n",
      " [ 14   4   2   0   5   7   0   0   3   2   0 155   5]\n",
      " [ 31   9   5   1   2  28   0   1  50  24   3   3  37]]\n",
      "\n",
      "classification_report is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.07      0.38      0.11        65\n",
      "           2       0.05      0.02      0.03        58\n",
      "           3       0.66      0.43      0.52        77\n",
      "           4       0.42      0.21      0.28        73\n",
      "           5       0.46      0.13      0.20        94\n",
      "           6       0.16      0.18      0.17        92\n",
      "           7       0.68      0.23      0.34       102\n",
      "           8       0.43      0.44      0.44       107\n",
      "           9       0.31      0.61      0.41       107\n",
      "          10       0.29      0.34      0.31       123\n",
      "          11       0.60      0.09      0.16       160\n",
      "          12       0.94      0.79      0.86       197\n",
      "          13       0.27      0.19      0.22       194\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      1449\n",
      "   macro avg       0.41      0.31      0.31      1449\n",
      "weighted avg       0.46      0.34      0.35      1449\n",
      "\n",
      "train Accuracy: 0.975833\n",
      "test Accuracy: 0.336094\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "prediction=mlp.predict(test)\n",
    "cm=metrics.confusion_matrix(ltest,prediction)\n",
    "print('confusion matrix is:')\n",
    "print(cm)\n",
    "print()\n",
    "print('classification_report is:')\n",
    "print(metrics.classification_report(ltest, prediction))#ngram(1,2 without to dense)\n",
    "print(\"train Accuracy: %f\" %mlp.score(train,ltrain))\n",
    "print(\"test Accuracy: %f\" %mlp.score(test,ltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.975833\n",
      "test Accuracy: 0.336094\n"
     ]
    }
   ],
   "source": [
    "#ngram(1,2 without to dense)\n",
    "print(\"train Accuracy: %f\" %mlp.score(train,ltrain))\n",
    "print(\"test Accuracy: %f\" %mlp.score(test,ltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "#import os, sys\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "##from sklearn.metrics import roc_curve, auc\n",
    "#from sklearn import svm\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import KeyedVectors\n",
    "#from scipy import spatial\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from nltk import bigrams\n",
    "import nltk\n",
    "import re\n",
    "#import spacy\n",
    "###FUNCTIONS\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "        words = sentence.split()\n",
    "        feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "        n_words = 0\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                n_words += 1\n",
    "                feature_vec = np.add(feature_vec, model[word])\n",
    "        if (n_words > 0):\n",
    "            feature_vec = np.divide(feature_vec, n_words)\n",
    "        return feature_vec\n",
    "\n",
    "def convert_word2vec(sentences,num_features):\n",
    "        vectors_matrix = np.zeros((len(sentences),num_features))\n",
    "        i = 0\n",
    "        for sent in sentences:\n",
    "            temp = avg_feature_vector(sent,word2vec_vectors,num_features,index2word_set)\n",
    "            vectors_matrix[i,:] = temp\n",
    "            i = i+1\n",
    "        return vectors_matrix\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    #txt = [token for token in doc ]\n",
    "    words=str(doc).strip().split()\n",
    "    #words=[st.stem(w) for w in words ]\n",
    "    return ' '.join(words)\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    #if len(txt) > 2:\n",
    "      #return ' '.join(txt)\n",
    "#from gensim.models.phrases import Phrases, Phraser\n",
    "#data=pd.read_excel('ml2.xlsx', header=None) ## read le file\n",
    "#data.columns=['text','label']\n",
    "#def prep(doc):\n",
    " #   words=str(doc).strip().split()\n",
    "  #  return ' '.join(words)\n",
    "#Xt=Xt.apply(prep)\n",
    "#sentences = Xt\n",
    "data=Xtb\n",
    "#data = data[data['Semantic_Type']!='DISREGARD']\n",
    "#from gensim.models.phrases import Phrases, Phraser\n",
    "#from nltk.stem import ISRIStemmer\n",
    "#st=ISRIStemmer()\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in data)\n",
    "txt = [cleaning(doc) for doc in data]\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape\n",
    "sent = [row.split() for row in df_clean['clean']]\n",
    "sentences = sent\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_vectors = Word2Vec(min_count=2,window=5,size=200,sample=6e-5,alpha=0.03,min_alpha=0.0007,negative=20)\n",
    "word2vec_vectors.build_vocab(sentences)\n",
    "word2vec_vectors.train(sentences, total_examples=word2vec_vectors.corpus_count, epochs=30, report_delay=1)\n",
    "word2vec_vectors.init_sims(replace=True)\n",
    "index2word_set = set(word2vec_vectors.wv.index2word)\n",
    "###vectorizing the sentences\n",
    "#vectors_matrix = convert_word2vec(data['Sentence'],200)\n",
    "#np.random.seed(10)\n",
    "#labels=data['label']\n",
    "#train,test,ltrain,ltest =train_test_split(vectors_matrix, labels,test_size=0.2)#,random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "صيد 0.9998825788497925\n",
      "حلاوي 0.9998754858970642\n",
      "الساتر 0.9998711347579956\n",
      "لبنانين 0.9998612403869629\n",
      "علام 0.9998611211776733\n",
      "اللبان 0.9998595118522644\n",
      "بسبب 0.9998589754104614\n",
      "بندقية 0.999858021736145\n",
      "الضرب 0.9998571276664734\n",
      "خلع 0.9998557567596436\n"
     ]
    }
   ],
   "source": [
    "t_model = word2vec_vectors\n",
    "pos_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['شك دون رصيد', 'سلاح حربي'] if t.strip() != \"\"]\n",
    "neg_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['موقوف'] if t.strip() != \"\"]\n",
    "\n",
    "vec = calc_vec(pos_tokens=pos_tokens, neg_tokens=neg_tokens, n_model=t_model, dim=t_model.vector_size)\n",
    "\n",
    "most_sims = t_model.wv.similar_by_vector(vec, topn=10)\n",
    "for term, score in most_sims:\n",
    "    if term not in pos_tokens+neg_tokens:\n",
    "        print(term, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مجهولة 0.9997663497924805\n",
      "اقدم 0.9997431039810181\n",
      "نارية 0.9996970891952515\n",
      "متن 0.9995927810668945\n",
      "دراجة 0.9994726777076721\n",
      "على 0.9993910789489746\n",
      "المواصفات 0.9993548393249512\n",
      "صغيرة 0.9992062449455261\n",
      "الخليوي 0.9989694356918335\n",
      "هاتفه 0.998902440071106\n",
      "مجهولان 0.9987654685974121\n",
      "الحجم 0.9986586570739746\n",
      "نوع 0.9980621337890625\n",
      "شخص 0.9978218674659729\n",
      "الاقدام 0.9974756240844727\n",
      "سامسونغ 0.99730384349823\n",
      "لون 0.9972778558731079\n",
      "هاتفها 0.9972022771835327\n",
      "هاتف 0.9970158338546753\n",
      "اثناء 0.9969624280929565\n",
      "الساعة 0.9968664646148682\n",
      "جادة 0.9968240261077881\n",
      "في 0.9966819286346436\n",
      "سيرا 0.9966714382171631\n",
      "فر 0.9963237643241882\n",
      "الى 0.996174156665802\n",
      "جهة 0.9961203336715698\n",
      "حوالي 0.9961131811141968\n",
      "اسود 0.9960875511169434\n",
      "الطيونة 0.995947003364563\n",
      "من 0.9959264993667603\n",
      "ايفون 0.9958641529083252\n",
      "باتجاه 0.9957720637321472\n",
      "التعرف 0.9957240223884583\n",
      "محلة 0.995665431022644\n",
      "ضد 0.9953701496124268\n",
      "خليوي 0.9952776432037354\n",
      "لا 0.9949822425842285\n",
      "مجهولين 0.9949564933776855\n",
      "المقدم 0.9948563575744629\n",
      "الفوري 0.9948371052742004\n",
      "بداخله 0.9945904612541199\n",
      "انه 0.9945818781852722\n",
      "باقي 0.9945602416992188\n",
      "حول 0.9944614171981812\n",
      "فرا 0.9943295121192932\n",
      "ادعى 0.99423748254776\n",
      "بيهم 0.9942246079444885\n",
      "الصلح 0.9941929578781128\n",
      "بالادعاء 0.9941902160644531\n",
      "داخل 0.9941717386245728\n",
      "شريحة 0.9941613674163818\n",
      "بدارو 0.9940956830978394\n",
      "النبع 0.9940576553344727\n",
      "راس 0.9939113259315491\n",
      "قصقص 0.993849515914917\n",
      "جهاز 0.9936628937721252\n",
      "شاتيلا 0.9936498403549194\n",
      "حيث 0.9936037063598633\n",
      "مستديرة 0.9934508800506592\n",
      "اطلعنا 0.9934120178222656\n",
      "تحقيق 0.993148684501648\n",
      "اوراق 0.993075966835022\n",
      "المدعية 0.9929251670837402\n",
      "سرقة 0.9929025173187256\n",
      "المحامي 0.9928020238876343\n",
      "اللوحة 0.9927911758422852\n",
      "يتخذ 0.9927675724029541\n",
      "ثبوتية 0.9927279949188232\n",
      "مجهول 0.9927030205726624\n",
      "حرش 0.9926848411560059\n",
      "بطريقة 0.9926698207855225\n",
      "العدلية 0.9926368594169617\n",
      "اخر 0.992392897605896\n",
      "بالاستدعاء 0.9923141002655029\n",
      "يستطيع 0.9922987222671509\n",
      "المدعوة 0.9922393560409546\n",
      "الشخصي 0.9922142624855042\n",
      "كان 0.9921903610229492\n",
      "يمكنه 0.992139458656311\n",
      "تاريخه 0.9921349287033081\n",
      "بادعاء 0.9920991063117981\n",
      "ذلك 0.9920927286148071\n",
      "قرب 0.9920287728309631\n",
      "مروره 0.9920071959495544\n",
      "الهاتف 0.9920026659965515\n",
      "سامي 0.991917610168457\n",
      "بعكس 0.9918650388717651\n",
      "فقدان 0.9917582273483276\n",
      "عليه 0.9916608333587646\n",
      "انها 0.9916200637817383\n",
      "حقيبتها 0.9916125535964966\n",
      "الموضوع 0.9916040897369385\n",
      "سيارته 0.9915720224380493\n",
      "صفة 0.9915251731872559\n",
      "المدعى 0.9915122985839844\n",
      "فاشار 0.9914878010749817\n",
      "لم 0.9914414286613464\n",
      "بموضوع 0.9913991689682007\n",
      "تحمل 0.9912859201431274\n"
     ]
    }
   ],
   "source": [
    "#python 3.X\n",
    "token = clean_str(u'نشل')\n",
    "# python 2.7\n",
    "# token = clean_str('تونس'.decode('utf8', errors='ignore'))\n",
    "\n",
    "most_similar = t_model.wv.most_similar( token, topn=100 )\n",
    "for term, score in most_similar:\n",
    "    print(term, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "def get_vec(n_model,dim, token):\n",
    "    vec = np.zeros(dim)\n",
    "    is_vec = False\n",
    "    if token not in n_model.wv:\n",
    "        _count = 0\n",
    "        is_vec = True\n",
    "        for w in token.split(\"_\"):\n",
    "            if w in n_model.wv:\n",
    "                _count += 1\n",
    "                vec += n_model.wv[w]\n",
    "        if _count > 0:\n",
    "            vec = vec / _count\n",
    "    else:\n",
    "        vec = n_model.wv[token]\n",
    "    return vec\n",
    "\n",
    "def calc_vec(pos_tokens, neg_tokens, n_model, dim):\n",
    "    vec = np.zeros(dim)\n",
    "    for p in pos_tokens:\n",
    "        vec += get_vec(n_model,dim,p)\n",
    "    for n in neg_tokens:\n",
    "        vec -= get_vec(n_model,dim,n)\n",
    "    \n",
    "    return vec   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['محضر', 'تحقيق', 'بادعاء', 'المدعوة', 'اسما', 'هدى', 'انيس', 'قساطلي', 'لبنانية', 'بحق', 'مجهول', 'بموضوع', 'فقدان', 'محفظة', 'تحتوي', 'بطاقة', 'هوية', 'و', 'مبلغ', 'مالي', '.']\n",
      "محضر تحقيق بادعاء المدعوة اسما هدى انيس قساطلي لبنانية بحق مجهول بموضوع فقدان محفظة تحتوي بطاقة هوية و مبلغ مالي .\n"
     ]
    }
   ],
   "source": [
    "print(sent[1])\n",
    "print(df_clean['clean'][1])\n",
    "#print(index2word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "Vectors_matrix = convert_word2vec(str(Xtb), 200)\n",
    "np.random.seed(10)\n",
    "#labels=Yt\n",
    "#train,test,ltrain,ltest =train_test_split(vectors_matrix, labels,test_size=0.2)#,random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.19547369e-03  5.82107157e-03 -3.37295979e-02  2.58115376e-03\n",
      " -5.45239747e-02 -1.11295097e-02  2.23629288e-02 -9.70533639e-02\n",
      "  5.43381870e-02  1.15961693e-01 -1.67765290e-01  6.15069270e-02\n",
      "  9.10789445e-02 -6.18498735e-02  1.28274396e-01 -1.32828997e-02\n",
      " -4.23808135e-02  5.98876849e-02 -1.68288082e-01 -6.40497496e-03\n",
      "  4.03265283e-02 -5.34946984e-03 -1.20958947e-02  1.27212465e-01\n",
      " -2.70150192e-02  5.80591895e-03 -8.85659307e-02  3.66403833e-02\n",
      "  7.43004084e-02  2.79699396e-02 -6.99312687e-02  7.49559104e-02\n",
      " -2.35138517e-02 -1.46704212e-01  7.79712871e-02  3.86479683e-02\n",
      " -9.15474743e-02  1.18426129e-03  1.00725638e-02  1.45641640e-01\n",
      "  9.51596797e-02  1.49067804e-01 -4.20593619e-02 -1.46789134e-01\n",
      "  1.11378260e-01  4.12388965e-02 -1.62494816e-02  6.38975054e-02\n",
      " -1.28784731e-01  3.79103459e-02 -4.01398055e-02  1.58800872e-03\n",
      "  1.82550922e-02 -1.32319018e-01 -4.35764156e-02  9.50044841e-02\n",
      " -1.70010403e-01 -2.86953803e-02  2.02054717e-02  2.56861318e-02\n",
      " -1.01169966e-01 -4.16843733e-03  4.04385384e-03  8.67961720e-02\n",
      "  5.67207113e-03 -3.29371020e-02 -1.11667648e-01  1.52746394e-01\n",
      "  2.38177758e-02  2.50647292e-02  5.87506257e-02  2.59194486e-02\n",
      " -4.33931574e-02 -2.84972321e-02 -5.21255806e-02  4.72321510e-02\n",
      " -1.49643412e-02  4.45475802e-02 -5.05417101e-02  5.62943742e-02\n",
      " -3.19287069e-02  2.91789696e-02 -7.96991810e-02 -6.06839247e-02\n",
      "  3.94682921e-02 -1.00327559e-01 -4.70448472e-03 -1.22331031e-01\n",
      " -1.55161768e-02  7.43733123e-02 -9.34893116e-02  9.85433906e-02\n",
      "  4.28674882e-03 -4.45325151e-02 -3.99576984e-02  7.63932914e-02\n",
      "  1.28972251e-02 -3.60970683e-02  8.02477300e-02  1.27256766e-01\n",
      " -2.39403211e-02 -4.69989367e-02  3.63007337e-02  1.18652377e-02\n",
      "  4.51930799e-02  6.88063502e-02  5.15041547e-03  2.57807914e-02\n",
      "  1.02394450e-04  6.75002262e-02  6.01080656e-02 -9.57538001e-03\n",
      "  5.93877770e-02  5.47390394e-02  1.32679701e-01 -4.86527197e-03\n",
      " -3.73045206e-02  4.35009561e-02  1.18667386e-01  1.32130012e-02\n",
      " -5.53853922e-02 -1.07975001e-03 -3.76958065e-02 -5.21113127e-02\n",
      "  6.06016442e-02  2.16881763e-02 -8.89433920e-02  4.39945003e-03\n",
      "  4.08999398e-02 -3.40677984e-02  2.87881959e-02 -1.18753472e-02\n",
      "  3.10503375e-02  6.47389665e-02  1.46759793e-01  4.78283735e-03\n",
      " -6.83225766e-02  7.45650604e-02 -1.11852340e-01 -4.30738069e-02\n",
      " -1.33047640e-01  8.75815153e-02 -1.08636655e-01 -6.22772053e-02\n",
      " -1.21547073e-01  5.85817620e-02 -3.75878550e-02  1.23675033e-01\n",
      "  8.17659646e-02  2.55654007e-02  7.91934878e-03 -8.85255635e-02\n",
      "  4.85880226e-02 -3.12523544e-02  7.11743459e-02  1.19776182e-01\n",
      " -8.22497830e-02  6.38516620e-02 -1.85670286e-01  5.54984510e-02\n",
      "  7.79012293e-02  7.51828775e-02  5.39621860e-02  1.41593525e-02\n",
      " -1.70663595e-02 -1.36548169e-02  4.58595753e-02  8.81101191e-02\n",
      "  4.34355922e-02  2.50473022e-02  1.13227390e-01 -3.85816656e-02\n",
      "  1.21502846e-01 -1.03183024e-01 -2.93460861e-02  6.35390915e-03\n",
      " -9.97771174e-02 -1.73141365e-03  4.75072078e-02 -6.09258888e-03\n",
      "  3.61997001e-02 -4.35612984e-02  1.43838450e-01  6.35560369e-03\n",
      "  5.14263613e-03  3.29888575e-02  5.32949120e-02 -5.16598932e-02\n",
      "  7.41354451e-02 -2.55965423e-02  6.57750815e-02 -2.21620947e-02\n",
      "  3.15903611e-02 -4.48293202e-02 -6.69704601e-02  2.75700577e-02\n",
      "  1.38729122e-02 -1.00098178e-01 -5.19680418e-02 -7.13002682e-02]\n"
     ]
    }
   ],
   "source": [
    "print(vectors_matrix[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11576431 -0.01002609 -0.03963389 -0.02863568 -0.10128719  0.00106025\n",
      " -0.09017426 -0.05331455 -0.0419889  -0.13812898  0.09951101 -0.0381453\n",
      " -0.01462054 -0.01923087 -0.08817789 -0.08783196  0.03045108 -0.04160171\n",
      " -0.08114232 -0.04087094  0.08253521  0.0815117   0.02126598 -0.01539329\n",
      " -0.00369903  0.03524168 -0.07055686 -0.006049   -0.01524615 -0.02146595\n",
      "  0.00159274  0.01183337  0.11407945 -0.10491984  0.03131336  0.03921975\n",
      "  0.11588425  0.02416547  0.01223431 -0.01914342  0.0303671   0.02794879\n",
      "  0.02775425 -0.01011413  0.02901007 -0.20301579  0.01415297  0.04110276\n",
      " -0.06836757  0.10523292 -0.04657363  0.00712253 -0.00623632 -0.08992033\n",
      " -0.15616646 -0.0813691   0.13406484 -0.06534479  0.07766499  0.07123289\n",
      " -0.04893962  0.00060352  0.02425328 -0.01052863 -0.00402694  0.0987502\n",
      " -0.03591302 -0.09796246  0.0273818   0.01614556 -0.05893252  0.00161612\n",
      " -0.03223323 -0.01235233 -0.05164547 -0.04085043 -0.04236106  0.01887214\n",
      " -0.08769996  0.10979282 -0.03733867  0.06459425  0.0904599  -0.00316583\n",
      " -0.02789096  0.05484002  0.05001674  0.05198313  0.00696132  0.01034531\n",
      " -0.04635666  0.08112239  0.05475089 -0.00258124 -0.04757192 -0.00171837\n",
      "  0.0786109  -0.0420994   0.00688542  0.17650889 -0.03592404 -0.09899215\n",
      "  0.02785198 -0.04080471  0.01450887  0.06381165 -0.07330924 -0.06779417\n",
      " -0.11155217  0.11820005  0.04488814  0.02818815 -0.07954443 -0.01242578\n",
      " -0.00171332  0.06835654 -0.02268185  0.0870454   0.00709698 -0.0068594\n",
      "  0.12124067  0.01834066  0.03600679 -0.0915285   0.03993406 -0.10611488\n",
      "  0.00136671 -0.12264579  0.09073062  0.10506505 -0.13577907 -0.00154851\n",
      "  0.02930753  0.11034149 -0.0689938   0.02690828  0.10464329  0.06055791\n",
      " -0.17681952 -0.05121326 -0.01768573  0.04000609 -0.0022592   0.21017483\n",
      "  0.05974077  0.13427529 -0.14403774  0.0487619  -0.00835285  0.02189881\n",
      " -0.01942188  0.00972023  0.06849947  0.01596223 -0.1869887  -0.06404559\n",
      "  0.04143201  0.04147428  0.005418   -0.13402511  0.0077327   0.02400275\n",
      " -0.00237897  0.01968412 -0.02491439  0.05309077  0.05285747 -0.07878394\n",
      "  0.18687184  0.01206227 -0.09482986 -0.10953297 -0.00679006 -0.06082544\n",
      " -0.08689678 -0.05269369 -0.02262811 -0.12611344  0.04119577 -0.07502618\n",
      "  0.02936299 -0.0661515   0.04424183  0.08615878 -0.00714719  0.11221454\n",
      "  0.0353883   0.06657637 -0.03983626  0.06498737 -0.03671875  0.01719549\n",
      " -0.0640747  -0.03882197 -0.04554718 -0.0346664   0.13320017 -0.01371052\n",
      "  0.04547972 -0.0711937 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors_matrix[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vectors_matrix[10:5000]\n",
    "#print(X_train[1])\n",
    "Y_train=Ytb[10:5000]\n",
    "#print(Y_train[10])\n",
    "X_test=vectors_matrix[5001:]\n",
    "Y_test=Ytb[5001:]\n",
    "#print(X_test[10])\n",
    "#print(Y_test[5001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3577, 1980]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-b88f460d2e7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mYtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mltrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mltest\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVectors_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#,random_state=8)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2184\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3577, 1980]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels=Ytb\n",
    "train,test,ltrain,ltest =train_test_split(Vectors_matrix, labels,test_size=0.2)#,random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 7. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-6a2f470c017c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    975\u001b[0m         \"\"\"\n\u001b[0;32m    976\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 977\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    322\u001b[0m                              hidden_layer_sizes)\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 914\u001b[1;33m                          multi_output=True)\n\u001b[0m\u001b[0;32m    915\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n\u001b[1;32m--> 759\u001b[1;33m                         dtype=None)\n\u001b[0m\u001b[0;32m    760\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 570\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             _assert_all_finite(array,\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 7. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(50,30,), max_iter=150, solver='adam',verbose =10 , tol=1e-4, random_state=2, learning_rate_init=0.01)\n",
    "model=mlp.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.000000\n",
      "test Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"train Accuracy: %f\" %mlp.score(X_train,Y_train))\n",
    "print(\"test Accuracy: %f\" %mlp.score(X_test,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
