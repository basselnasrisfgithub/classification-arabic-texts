{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tags                                               post\n",
      "1    11  عند التاسعة من مساء يوم 29/9/2018 وفي محلة الط...\n",
      "2     4  اقدم المدعو وليد عدنان مرعي على شتم و اهانة ال...\n",
      "3    12  أقدم المدعو ناصر رضا على اصدار شك لصالح حسن اح...\n",
      "4     1  الساعة 7.30 من تاريخه وفي محلة فردان شارع كرام...\n"
     ]
    }
   ],
   "source": [
    "#readind and preparing data ( text , label )\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_excel('incident13type.xlsx', header=None) ## read le file\n",
    "df.columns=['tags','post']\n",
    "print(df[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('arabic'))\n",
    "from bs4 import BeautifulSoup\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9ا-ي #+_]')\n",
    "STOPWORDS = set(stopwords.words('arabic'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['post'] = df['post'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التاسعة مسا يوم 29 9 2018 وفي محلة الطريق الجديدة ارض جلول اوقفت دورية مكتب معلومات بيروت الرابعة المدعو خالد عسكر الاشتباه بترويج المخدرات عثرت بحوزته كمية المخدرات الشكل التالي 50 حبة نوع بنزكسول80 1 2 حبة نوع بنزكسول موضبين داخل ثلاثة اكياس صغيرة 9 حبات ونصف نوع ترامال19 حبة نوع المخدرة للاعصاب49 حبة مجهولة الصنف لون اصفر داخل حنجور بالاضافة الى 13 كارت نوع تيليكارت مغلفين\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التاسعة مسا يوم           وفي محلة الطريق الجديدة ارض جلول اوقفت دورية مكتب معلومات بيروت الرابعة المدعو خالد عسكر الاشتباه بترويج المخدرات عثرت بحوزته كمية المخدرات الشكل التالي    حبة نوع بنزكسول       حبة نوع بنزكسول موضبين داخل ثلاثة اكياس صغيرة   حبات ونصف نوع ترامال   حبة نوع المخدرة للاعصاب   حبة مجهولة الصنف لون اصفر داخل حنجور بالاضافة الى    كارت نوع تيليكارت مغلفين\n"
     ]
    }
   ],
   "source": [
    "print(df['post'][1])\n",
    "table=str.maketrans(\"+=_-<>~/!*%?:;,)&1234567890@$qwertyuioplkjhgfdsamnbvcxz(QWERTYUIOPLKJHGFDSAZXCVBNM\",82*\" \")\n",
    "for i in range(len(df['post'])):\n",
    "    df['post'][i] = df['post'][i].translate(table)\n",
    "print(df['post'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Arabic Transliteration based on Buckwalter\n",
    "# dictionary source is buckwalter2unicode.py http://www.redhat.com/archives/fedora-extras-commits/2007-June/msg03617.html \n",
    "\n",
    "buck2uni = {\"'\": u\"\\u0621\", # hamza-on-the-line\n",
    "            \"|\": u\"\\u0622\", # madda\n",
    "            \">\": u\"\\u0623\", # hamza-on-'alif\n",
    "            \"&\": u\"\\u0624\", # hamza-on-waaw\n",
    "            \"<\": u\"\\u0625\", # hamza-under-'alif\n",
    "            \"}\": u\"\\u0626\", # hamza-on-yaa'\n",
    "            \"A\": u\"\\u0627\", # bare 'alif\n",
    "            \"b\": u\"\\u0628\", # baa'\n",
    "            \"p\": u\"\\u0629\", # taa' marbuuTa\n",
    "            \"t\": u\"\\u062A\", # taa'\n",
    "            \"v\": u\"\\u062B\", # thaa'\n",
    "            \"j\": u\"\\u062C\", # jiim\n",
    "            \"H\": u\"\\u062D\", # Haa'\n",
    "            \"x\": u\"\\u062E\", # khaa'\n",
    "            \"d\": u\"\\u062F\", # daal\n",
    "            \"*\": u\"\\u0630\", # dhaal\n",
    "            \"r\": u\"\\u0631\", # raa'\n",
    "            \"z\": u\"\\u0632\", # zaay\n",
    "            \"s\": u\"\\u0633\", # siin\n",
    "            \"$\": u\"\\u0634\", # shiin\n",
    "            \"S\": u\"\\u0635\", # Saad\n",
    "            \"D\": u\"\\u0636\", # Daad\n",
    "            \"T\": u\"\\u0637\", # Taa'\n",
    "            \"Z\": u\"\\u0638\", # Zaa' (DHaa')\n",
    "            \"E\": u\"\\u0639\", # cayn\n",
    "            \"g\": u\"\\u063A\", # ghayn\n",
    "            \"_\": u\"\\u0640\", # taTwiil\n",
    "            \"f\": u\"\\u0641\", # faa'\n",
    "            \"q\": u\"\\u0642\", # qaaf\n",
    "            \"k\": u\"\\u0643\", # kaaf\n",
    "            \"l\": u\"\\u0644\", # laam\n",
    "            \"m\": u\"\\u0645\", # miim\n",
    "            \"n\": u\"\\u0646\", # nuun\n",
    "            \"h\": u\"\\u0647\", # haa'\n",
    "            \"w\": u\"\\u0648\", # waaw\n",
    "            \"Y\": u\"\\u0649\", # 'alif maqSuura\n",
    "            \"y\": u\"\\u064A\", # yaa'\n",
    "            \"F\": u\"\\u064B\", # fatHatayn\n",
    "            \"N\": u\"\\u064C\", # Dammatayn\n",
    "            \"K\": u\"\\u064D\", # kasratayn\n",
    "            \"a\": u\"\\u064E\", # fatHa\n",
    "            \"u\": u\"\\u064F\", # Damma\n",
    "            \"i\": u\"\\u0650\", # kasra\n",
    "            \"~\": u\"\\u0651\", # shaddah\n",
    "            \"o\": u\"\\u0652\", # sukuun\n",
    "            \"`\": u\"\\u0670\", # dagger 'alif\n",
    "            \"{\": u\"\\u0671\", # waSla\n",
    "}\n",
    "def transString(string, reverse=0):\n",
    "    '''Given a Unicode string, transliterate into Buckwalter. To go from\n",
    "    Buckwalter back to Unicode, set reverse=1'''\n",
    "\n",
    "    for k, v in buck2uni.items():\n",
    "      if not reverse:\n",
    "            string = string.replace(v, k)\n",
    "      else:\n",
    "            string = string.replace(k, v)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df['post'])):\n",
    "   df['post'][i]=transString(str(df['post'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['AlsAEp', 'tAryxh', 'ASTdmt', 'AldrAjp', 'AlnAryp', 'nwE', 'lyn', 'hAy', 'lwn', 'mwf', 'SnE', 'rqm', 'm', 'bqyAdp', 'AlmdEw', 'mHmd', 'AHmd', 'EwADp', 'wAldth', 'HnAn', 'twld', 'lbnAny', 'bfAn', 'nwE', 'mAzdA', 'SnE', 'lwn', 'AbyD', 'rqm', 'm', 'bqyAdp', 'Ely', 'EbAs', 'zEytr', 'wAldth', 'nwr', 'twld', 'lbnAny', 'ntj', 'AlHAdv', 'ASAbp', 'AlAwl', 'bkswr', 'Alwjh', 'nql', 'AlY', 'mst$fY', 'AljAmEp', 'AlAmyrykyp', 'llmEAljp', 'nZm', 'mHDr', 'rqm'], tags=['Train_0']),\n",
       " TaggedDocument(words=['slmtnA', 'dwryp', 'fSyltnA', 'AlmdEw', 'xAld', 'EysY', 'swry', 'bjrm', 'dxwl', 'AlblAd', 'xlsp', 'w*lk', 'twqyfh', 'mHlp', 'Alrw$p', 'AlsAEp'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2383036.40it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2385879.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2417022.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2377370.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2371731.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1191234.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2414104.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2379633.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2423466.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2419946.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1192797.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2349440.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1191944.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1191092.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2356639.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2388158.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2388158.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1192939.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2383604.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2348337.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1190666.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2419946.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2385310.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2381901.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2372857.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2384172.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2381901.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1190950.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 1193224.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2384/2384 [00:00<00:00, 2350545.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.776536312849162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.77      0.73        83\n",
      "           2       0.54      0.48      0.51        31\n",
      "           3       0.94      0.93      0.94       120\n",
      "           4       0.39      0.52      0.45        21\n",
      "           5       0.44      0.53      0.48        15\n",
      "           6       0.31      0.20      0.24        20\n",
      "           7       0.89      0.79      0.84       150\n",
      "           8       0.93      0.93      0.93        15\n",
      "           9       0.78      0.90      0.84        62\n",
      "          10       0.50      0.49      0.49        39\n",
      "          11       0.98      0.89      0.93        63\n",
      "          12       0.86      0.96      0.91        25\n",
      "          13       0.74      0.75      0.74        72\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       716\n",
      "   macro avg       0.69      0.70      0.69       716\n",
      "weighted avg       0.78      0.78      0.78       716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
