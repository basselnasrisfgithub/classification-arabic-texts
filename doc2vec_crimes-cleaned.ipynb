{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tags                                               post\n",
      "1    12  محضر تحقيق بالشكوى المقدمة بنك الشرق ضد جوزف ع...\n",
      "2    13      حول ادعاء طوني يارد ضد مجهول بجرم سرقة محفظته\n",
      "3    11  محضر تحقيق فوري بتوقيف اللبناني فارس ورده بجرم...\n",
      "4    12  محضر تحقيق بإدعاء حسام حسين قبيسي ضد المدعى عل...\n"
     ]
    }
   ],
   "source": [
    "#readind and preparing data ( text , label )\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_excel('ml2.xlsx', header=None) ## read le file\n",
    "df.columns=['tags','post']\n",
    "print(df[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('arabic'))\n",
    "from bs4 import BeautifulSoup\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9ا-ي #+_]')\n",
    "STOPWORDS = set(stopwords.words('arabic'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['post'] = df['post'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "محضر تحقيق بالشكوى المقدمة بنك الشرق ضد جوزف عبدو سعد بجرم شك رصيد\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "محضر تحقيق بالشكوى المقدمة بنك الشرق ضد جوزف عبدو سعد بجرم شك رصيد\n"
     ]
    }
   ],
   "source": [
    "print(df['post'][1])\n",
    "table=str.maketrans(\"+=_-<>~/!*%?:;,)&1234567890@$qwertyuioplkjhgfdsamnbvcxz(QWERTYUIOPLKJHGFDSAZXCVBNM\",82*\" \")\n",
    "for i in range(len(df['post'])):\n",
    "    df['post'][i] = df['post'][i].translate(table)\n",
    "print(df['post'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['تحقيق', 'بادعا', 'رواد', 'ربيع', 'داغر', 'ضد', 'عدد', 'الشخاص', 'بجرم', 'ضرب', 'وايذا'], tags=['Train_0']),\n",
       " TaggedDocument(words=['الساعة', 'تاريخه', 'وفي', 'محلة', 'مار', 'الياس', 'امام', 'سنتر', 'المقاصد', 'حصل', 'حادث', 'اصطدام', 'سيارة', 'نوع', 'مرسيدس', 'رقم', 'ج', 'بقيادة', 'ندين', 'خضر', 'الجمل', 'والدتها', 'سوسن', 'تولد', 'لبنانية', 'ودراجة', 'نارية', 'نوع', 'عقاد', 'رقم', 'م', 'بقيادة', 'كامل', 'راشد', 'جعفر', 'والدته', 'محموده', 'تولد', 'لبناني', 'نقل', 'الى', 'مستشفى', 'المقاصد', 'لاصابته', 'برضوض', 'وحالته', 'مستقرة'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1448643.15it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2415903.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2416287.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 3622126.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1813874.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2416864.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1441972.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1811819.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2426905.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2416479.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 3648228.39it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2422840.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2413791.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2416287.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2427293.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2400818.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2404619.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2415518.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2428651.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2428651.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2427487.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2428651.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1818326.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1802251.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1818217.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1818871.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2430789.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2415903.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 2416287.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1817782.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7242/7242 [00:00<00:00, 1806216.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8104003681546249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.68      0.66        93\n",
      "           2       0.70      0.55      0.62        98\n",
      "           3       0.88      0.83      0.86       129\n",
      "           4       0.54      0.60      0.57       115\n",
      "           5       0.74      0.71      0.73       139\n",
      "           6       0.56      0.55      0.55       116\n",
      "           7       0.90      0.84      0.87       139\n",
      "           8       0.97      0.97      0.97       158\n",
      "           9       0.78      0.88      0.83       164\n",
      "          10       0.71      0.72      0.71       200\n",
      "          11       0.97      0.98      0.98       231\n",
      "          12       0.94      0.98      0.96       290\n",
      "          13       0.82      0.78      0.80       301\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      2173\n",
      "   macro avg       0.78      0.78      0.78      2173\n",
      "weighted avg       0.81      0.81      0.81      2173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
