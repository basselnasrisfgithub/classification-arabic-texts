{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "import pyarabic.named as named\n",
    "#import normalize_hamza\n",
    "txt=data['text']\n",
    "txt[1].split()\n",
    "txt[1].named\n",
    "#normalize_hamza(txt[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smart_open.smart_open_lib:this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wv = gensim.models.Word2Vec.load('models/full_grams_cbow_100_twitter.mdl')\n",
    "from gensim.models import Word2Vec\n",
    "#wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تفائل',\n",
       " 'وصباح',\n",
       " 'حنه',\n",
       " 'ورنه',\n",
       " 'عارفين_نفسهم',\n",
       " 'اثير',\n",
       " 'اهوي',\n",
       " 'المشاك',\n",
       " 'تبتعد_عنه',\n",
       " 'بالم',\n",
       " 'قاع',\n",
       " 'واححد',\n",
       " 'ياورده',\n",
       " 'اكممل',\n",
       " 'وردده',\n",
       " 'كييس',\n",
       " 'مليان',\n",
       " 'هاللعبه',\n",
       " 'لول',\n",
       " 'خادم']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tags                                               post\n",
      "1     3  محضر تحقيق بادعاء المدعوة اسما هدى انيس قساطلي...\n",
      "2     4  محضر تحقيق بحق المدعو ماهر صالح الحرامي سوري ل...\n",
      "3     1  محضر بتنفيذ خلاصة حكم بحق المدعو محمد سعد الدي...\n",
      "4     2  محضر بتنفيذ قرار جزائي بحق المدعو محمد سعد الد...\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#readind and preparing data ( text , label )\n",
    "df=pd.read_excel('ML1.xlsx', header=None) ## read le file\n",
    "df.columns=['tags','post']\n",
    "print(df[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['محضر', 'تحقيق', 'بالشكوى', 'المقدمة', 'من', 'ليال', 'حماده', 'ضد', 'المؤهل', 'مهدي', 'ظنيط', 'في', 'الامن', 'العام', 'بجرم', 'استيلاء', 'على', 'سيارتها', 'تسليمها', 'تركه', 'سند', 'اقامة']\n"
     ]
    }
   ],
   "source": [
    "print(test_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train_word_average = word_averaging_list(wv.wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv.wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
    "y_pred = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9175084175084175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.71      0.74        70\n",
      "           1       0.96      0.98      0.97        94\n",
      "           2       0.99      0.98      0.99       103\n",
      "           3       0.96      0.95      0.96       110\n",
      "           4       0.89      0.91      0.90       217\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       594\n",
      "   macro avg       0.91      0.91      0.91       594\n",
      "weighted avg       0.92      0.92      0.92       594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
    "print(classification_report(test.tags, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "def get_vec(n_model,dim, token):\n",
    "    vec = np.zeros(dim)\n",
    "    is_vec = False\n",
    "    if token not in n_model.wv:\n",
    "        _count = 0\n",
    "        is_vec = True\n",
    "        for w in token.split(\"_\"):\n",
    "            if w in n_model.wv:\n",
    "                _count += 1\n",
    "                vec += n_model.wv[w]\n",
    "        if _count > 0:\n",
    "            vec = vec / _count\n",
    "    else:\n",
    "        vec = n_model.wv[token]\n",
    "    return vec\n",
    "\n",
    "def calc_vec(pos_tokens, neg_tokens, n_model, dim):\n",
    "    vec = np.zeros(dim)\n",
    "    for p in pos_tokens:\n",
    "        vec += get_vec(n_model,dim,p)\n",
    "    for n in neg_tokens:\n",
    "        vec -= get_vec(n_model,dim,n)\n",
    "    \n",
    "    return vec   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ملغومه 0.5874800682067871\n",
      "ضخمه 0.5862234234809875\n",
      "عشوائيه 0.5828266143798828\n",
      "الناريه 0.5813839435577393\n",
      "تاريخيه 0.5780879855155945\n",
      "خطيره 0.5702124834060669\n",
      "غير_مسبوقه 0.5676120519638062\n",
      "مشبوهه 0.5671120882034302\n"
     ]
    }
   ],
   "source": [
    "t_model = wv\n",
    "pos_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['نارية', 'سرقة'] if t.strip() != \"\"]\n",
    "neg_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in ['نشل'] if t.strip() != \"\"]\n",
    "\n",
    "vec = calc_vec(pos_tokens=pos_tokens, neg_tokens=neg_tokens, n_model=t_model, dim=t_model.vector_size)\n",
    "\n",
    "most_sims = t_model.wv.similar_by_vector(vec, topn=10)\n",
    "for term, score in most_sims:\n",
    "    if term not in pos_tokens+neg_tokens:\n",
    "        print(term, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ينهاكم 0.630063533782959\n",
      "يصلح_عمل 0.5926780104637146\n",
      "شر_الدواب_عند 0.5893514156341553\n",
      "ما_قدروا 0.5666218996047974\n",
      "تربحهم 0.5639942288398743\n",
      "{كتب 0.5596617460250854\n",
      "الله_ورسوله_واتقوا 0.5563033223152161\n",
      "تعالي_يغار 0.5538311004638672\n",
      "وما_يذكرون 0.5401697158813477\n",
      "انما_نطعمكم_لوجه 0.5332059860229492\n"
     ]
    }
   ],
   "source": [
    "token = clean_str(u'جرم')\n",
    "\n",
    "most_similar = t_model.wv.most_similar( token, topn=10)\n",
    "for term, score in most_similar:\n",
    "    print(term, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
