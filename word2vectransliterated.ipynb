{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "1  محضر تحقيق بالشكوى المقدمة بنك الشرق ضد جوزف ع...     12\n",
      "2      حول ادعاء طوني يارد ضد مجهول بجرم سرقة محفظته     13\n",
      "3  محضر تحقيق فوري بتوقيف اللبناني فارس ورده بجرم...     11\n",
      "4  محضر تحقيق بإدعاء حسام حسين قبيسي ضد المدعى عل...     12\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#readind and preparing data ( text , label )\n",
    "data=pd.read_excel('ml2.xlsx', header=None) ## read le file\n",
    "data.columns=['text','label']\n",
    "print(data[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt=data['text']\n",
    "#X_train=Xt[0:5000]\n",
    "#print(X_train[1])\n",
    "Yt=data['label']\n",
    "#Y_train=Yt[0:5000]\n",
    "##print(Y_train[1])\n",
    "#X_test=Xt[5001:]\n",
    "#Y_test=Yt[5001:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Arabic Transliteration based on Buckwalter\n",
    "# dictionary source is buckwalter2unicode.py http://www.redhat.com/archives/fedora-extras-commits/2007-June/msg03617.html \n",
    "\n",
    "buck2uni = {\"'\": u\"\\u0621\", # hamza-on-the-line\n",
    "            \"|\": u\"\\u0622\", # madda\n",
    "            \">\": u\"\\u0623\", # hamza-on-'alif\n",
    "            \"&\": u\"\\u0624\", # hamza-on-waaw\n",
    "            \"<\": u\"\\u0625\", # hamza-under-'alif\n",
    "            \"}\": u\"\\u0626\", # hamza-on-yaa'\n",
    "            \"A\": u\"\\u0627\", # bare 'alif\n",
    "            \"b\": u\"\\u0628\", # baa'\n",
    "            \"p\": u\"\\u0629\", # taa' marbuuTa\n",
    "            \"t\": u\"\\u062A\", # taa'\n",
    "            \"v\": u\"\\u062B\", # thaa'\n",
    "            \"j\": u\"\\u062C\", # jiim\n",
    "            \"H\": u\"\\u062D\", # Haa'\n",
    "            \"x\": u\"\\u062E\", # khaa'\n",
    "            \"d\": u\"\\u062F\", # daal\n",
    "            \"*\": u\"\\u0630\", # dhaal\n",
    "            \"r\": u\"\\u0631\", # raa'\n",
    "            \"z\": u\"\\u0632\", # zaay\n",
    "            \"s\": u\"\\u0633\", # siin\n",
    "            \"$\": u\"\\u0634\", # shiin\n",
    "            \"S\": u\"\\u0635\", # Saad\n",
    "            \"D\": u\"\\u0636\", # Daad\n",
    "            \"T\": u\"\\u0637\", # Taa'\n",
    "            \"Z\": u\"\\u0638\", # Zaa' (DHaa')\n",
    "            \"E\": u\"\\u0639\", # cayn\n",
    "            \"g\": u\"\\u063A\", # ghayn\n",
    "            \"_\": u\"\\u0640\", # taTwiil\n",
    "            \"f\": u\"\\u0641\", # faa'\n",
    "            \"q\": u\"\\u0642\", # qaaf\n",
    "            \"k\": u\"\\u0643\", # kaaf\n",
    "            \"l\": u\"\\u0644\", # laam\n",
    "            \"m\": u\"\\u0645\", # miim\n",
    "            \"n\": u\"\\u0646\", # nuun\n",
    "            \"h\": u\"\\u0647\", # haa'\n",
    "            \"w\": u\"\\u0648\", # waaw\n",
    "            \"Y\": u\"\\u0649\", # 'alif maqSuura\n",
    "            \"y\": u\"\\u064A\", # yaa'\n",
    "            \"F\": u\"\\u064B\", # fatHatayn\n",
    "            \"N\": u\"\\u064C\", # Dammatayn\n",
    "            \"K\": u\"\\u064D\", # kasratayn\n",
    "            \"a\": u\"\\u064E\", # fatHa\n",
    "            \"u\": u\"\\u064F\", # Damma\n",
    "            \"i\": u\"\\u0650\", # kasra\n",
    "            \"~\": u\"\\u0651\", # shaddah\n",
    "            \"o\": u\"\\u0652\", # sukuun\n",
    "            \"`\": u\"\\u0670\", # dagger 'alif\n",
    "            \"{\": u\"\\u0671\", # waSla\n",
    "}\n",
    "def transString(string, reverse=0):\n",
    "    '''Given a Unicode string, transliterate into Buckwalter. To go from\n",
    "    Buckwalter back to Unicode, set reverse=1'''\n",
    "\n",
    "    for k, v in buck2uni.items():\n",
    "      if not reverse:\n",
    "            string = string.replace(v, k)\n",
    "      else:\n",
    "            string = string.replace(k, v)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Xt)):\n",
    "    Xt[i]=transString(Xt[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mHDr tHqyq bAdEA' fwAz mHmd Abw Emr/ lbnAny Dd mjhwlyn bjrm srqp bwAsTp Alksr wAlxlE. \n",
      "mHDr tHqyq bAdEA' fwAz mHmd Abw Emr/ lbnAny Dd mjhwlyn bjrm srqp bwAsTp Alksr wAlxlE. \n"
     ]
    }
   ],
   "source": [
    "len(Xt)\n",
    "print(data['text'][8])\n",
    "print(Xt[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2262e1e26e18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mSW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#ar_stemmer = stemmer(\"arabic\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#ar_stemmer.stemWord(u\"فسميتموها\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import stemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "SW=set(stopwords.words('english'))\n",
    "#ar_stemmer = stemmer(\"arabic\")\n",
    "#ar_stemmer.stemWord(u\"فسميتموها\")\n",
    "def prep(doc):\n",
    "    words=str(doc).strip().split()###  strip pr eliminer les white spaces, lower por transformer en miniscules\n",
    "    #stemmer=SnowballStemmer(\"arabic\")\n",
    "    #ar_stemmer = stemmer(\"arabic\")\n",
    "    #words=ar_stemmer.stem(w.decode('UTF-8'))\n",
    "    words=[stemmer.stem(w) for w in words  if not w in SW]## transformer en stem without les stopwords\n",
    "    return ' '.join(words)## met des spaces entre les mots\n",
    "xt=Xt.apply(prep)#### select le colomn text selement et la preper(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(doc):\n",
    "    words=str(doc).strip().split()\n",
    "    return ''.join(words)\n",
    "XT=Xt.apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model1=TfidfVectorizer()\n",
    "tfidf_matrix=model1.fit_transform(XT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels=data['label']\n",
    "train,test,ltrain,ltest =\\\n",
    "train_test_split(tfidf_matrix, labels,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.10071537\n",
      "Iteration 2, loss = 1.41568510\n",
      "Iteration 3, loss = 0.87649561\n",
      "Iteration 4, loss = 0.54118995\n",
      "Iteration 5, loss = 0.36983646\n",
      "Iteration 6, loss = 0.27539956\n",
      "Iteration 7, loss = 0.19484270\n",
      "Iteration 8, loss = 0.16269285\n",
      "Iteration 9, loss = 0.16689769\n",
      "Iteration 10, loss = 0.15249663\n",
      "Iteration 11, loss = 0.14887979\n",
      "Iteration 12, loss = 0.13974364\n",
      "Iteration 13, loss = 0.13645701\n",
      "Iteration 14, loss = 0.12231139\n",
      "Iteration 15, loss = 0.12030324\n",
      "Iteration 16, loss = 0.13419733\n",
      "Iteration 17, loss = 0.14809884\n",
      "Iteration 18, loss = 0.21203003\n",
      "Iteration 19, loss = 0.26580160\n",
      "Iteration 20, loss = 0.27561326\n",
      "Iteration 21, loss = 0.31295914\n",
      "Iteration 22, loss = 0.27506503\n",
      "Iteration 23, loss = 0.25585115\n",
      "Iteration 24, loss = 0.20318597\n",
      "Iteration 25, loss = 0.18493851\n",
      "Iteration 26, loss = 0.18097808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(50,30,), max_iter=120, solver='adam',verbose =10 , tol=1e-4, random_state=2, learning_rate_init=0.1)\n",
    "model=mlp.fit(train.todense(),ltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[ 25   0   1   0   0   2  11  14   3   4   0   0   5]\n",
      " [ 10   1   1   1   2   9   0   0  15   9   0   0  10]\n",
      " [ 10   2  33   0   0   3   0   0   9  12   3   0   5]\n",
      " [  9   1   2  15   1  11   0   0  10  14   0   0  10]\n",
      " [  7   2   0   1  12  26   0   0  14   9   0   6  17]\n",
      " [  9   1   0   8   4  17   0   2  20  15   2   1  13]\n",
      " [ 45   0   1   0   0   0  23  32   0   1   0   0   0]\n",
      " [ 60   0   0   0   0   0   0  47   0   0   0   0   0]\n",
      " [ 13   0   2   0   0   3   0   1  65  11   0   0  12]\n",
      " [ 19   0   2   7   0   2   0   2  23  42   2   0  24]\n",
      " [125   0   1   3   0   1   0  10   0   4  15   0   1]\n",
      " [ 14   4   2   0   5   7   0   0   3   2   0 155   5]\n",
      " [ 31   9   5   1   2  28   0   1  50  24   3   3  37]]\n",
      "\n",
      "classification_report is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.07      0.38      0.11        65\n",
      "           2       0.05      0.02      0.03        58\n",
      "           3       0.66      0.43      0.52        77\n",
      "           4       0.42      0.21      0.28        73\n",
      "           5       0.46      0.13      0.20        94\n",
      "           6       0.16      0.18      0.17        92\n",
      "           7       0.68      0.23      0.34       102\n",
      "           8       0.43      0.44      0.44       107\n",
      "           9       0.31      0.61      0.41       107\n",
      "          10       0.29      0.34      0.31       123\n",
      "          11       0.60      0.09      0.16       160\n",
      "          12       0.94      0.79      0.86       197\n",
      "          13       0.27      0.19      0.22       194\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      1449\n",
      "   macro avg       0.41      0.31      0.31      1449\n",
      "weighted avg       0.46      0.34      0.35      1449\n",
      "\n",
      "train Accuracy: 0.975833\n",
      "test Accuracy: 0.336094\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "prediction=mlp.predict(test)\n",
    "cm=metrics.confusion_matrix(ltest,prediction)\n",
    "print('confusion matrix is:')\n",
    "print(cm)\n",
    "print()\n",
    "print('classification_report is:')\n",
    "print(metrics.classification_report(ltest, prediction))#ngram(1,2 without to dense)\n",
    "print(\"train Accuracy: %f\" %mlp.score(train,ltrain))\n",
    "print(\"test Accuracy: %f\" %mlp.score(test,ltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.975833\n",
      "test Accuracy: 0.336094\n"
     ]
    }
   ],
   "source": [
    "#ngram(1,2 without to dense)\n",
    "print(\"train Accuracy: %f\" %mlp.score(train,ltrain))\n",
    "print(\"test Accuracy: %f\" %mlp.score(test,ltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "#import os, sys\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "##from sklearn.metrics import roc_curve, auc\n",
    "#from sklearn import svm\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import KeyedVectors\n",
    "#from scipy import spatial\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from nltk import bigrams\n",
    "import nltk\n",
    "import re\n",
    "#import spacy\n",
    "###FUNCTIONS\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "        words = sentence.split()\n",
    "        feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "        n_words = 0\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                n_words += 1\n",
    "                feature_vec = np.add(feature_vec, model[word])\n",
    "        if (n_words > 0):\n",
    "            feature_vec = np.divide(feature_vec, n_words)\n",
    "        return feature_vec\n",
    "\n",
    "def convert_word2vec(sentences,num_features):\n",
    "        vectors_matrix = np.zeros((len(sentences),num_features))\n",
    "        i = 0\n",
    "        for sent in sentences:\n",
    "            temp = avg_feature_vector(sent,word2vec_vectors,num_features,index2word_set)\n",
    "            vectors_matrix[i,:] = temp\n",
    "            i = i+1\n",
    "        return vectors_matrix\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token for token in doc ]\n",
    "    #words=str(doc).strip().split()\n",
    "    #words=[st.stem(w) for w in words ]\n",
    "    #return ' '.join(words)\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "      return ' '.join(txt)\n",
    "#from gensim.models.phrases import Phrases, Phraser\n",
    "#data=pd.read_excel('ml2.xlsx', header=None) ## read le file\n",
    "#data.columns=['text','label']\n",
    "#def prep(doc):\n",
    " #   words=str(doc).strip().split()\n",
    "  #  return ' '.join(words)\n",
    "#Xt=Xt.apply(prep)\n",
    "#sentences = Xt\n",
    "data=Xt\n",
    "#data = data[data['Semantic_Type']!='DISREGARD']\n",
    "#from gensim.models.phrases import Phrases, Phraser\n",
    "#from nltk.stem import ISRIStemmer\n",
    "#st=ISRIStemmer()\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in data)\n",
    "txt = [cleaning(doc) for doc in data]\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape\n",
    "sent = [row.split() for row in df_clean['clean']]\n",
    "sentences = sent\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_vectors = Word2Vec(min_count=2,window=5,size=200,sample=6e-5,alpha=0.03,min_alpha=0.0007,negative=20)\n",
    "word2vec_vectors.build_vocab(sentences)\n",
    "word2vec_vectors.train(sentences, total_examples=word2vec_vectors.corpus_count, epochs=30, report_delay=1)\n",
    "word2vec_vectors.init_sims(replace=True)\n",
    "index2word_set = set(word2vec_vectors.wv.index2word)\n",
    "###vectorizing the sentences\n",
    "#vectors_matrix = convert_word2vec(data['Sentence'],200)\n",
    "#np.random.seed(10)\n",
    "#labels=data['label']\n",
    "#train,test,ltrain,ltest =train_test_split(vectors_matrix, labels,test_size=0.2)#,random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "vectors_matrix = convert_word2vec(Xt,200)\n",
    "np.random.seed(10)\n",
    "#labels=Yt\n",
    "#train,test,ltrain,ltest =train_test_split(vectors_matrix, labels,test_size=0.2)#,random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.19547369e-03  5.82107157e-03 -3.37295979e-02  2.58115376e-03\n",
      " -5.45239747e-02 -1.11295097e-02  2.23629288e-02 -9.70533639e-02\n",
      "  5.43381870e-02  1.15961693e-01 -1.67765290e-01  6.15069270e-02\n",
      "  9.10789445e-02 -6.18498735e-02  1.28274396e-01 -1.32828997e-02\n",
      " -4.23808135e-02  5.98876849e-02 -1.68288082e-01 -6.40497496e-03\n",
      "  4.03265283e-02 -5.34946984e-03 -1.20958947e-02  1.27212465e-01\n",
      " -2.70150192e-02  5.80591895e-03 -8.85659307e-02  3.66403833e-02\n",
      "  7.43004084e-02  2.79699396e-02 -6.99312687e-02  7.49559104e-02\n",
      " -2.35138517e-02 -1.46704212e-01  7.79712871e-02  3.86479683e-02\n",
      " -9.15474743e-02  1.18426129e-03  1.00725638e-02  1.45641640e-01\n",
      "  9.51596797e-02  1.49067804e-01 -4.20593619e-02 -1.46789134e-01\n",
      "  1.11378260e-01  4.12388965e-02 -1.62494816e-02  6.38975054e-02\n",
      " -1.28784731e-01  3.79103459e-02 -4.01398055e-02  1.58800872e-03\n",
      "  1.82550922e-02 -1.32319018e-01 -4.35764156e-02  9.50044841e-02\n",
      " -1.70010403e-01 -2.86953803e-02  2.02054717e-02  2.56861318e-02\n",
      " -1.01169966e-01 -4.16843733e-03  4.04385384e-03  8.67961720e-02\n",
      "  5.67207113e-03 -3.29371020e-02 -1.11667648e-01  1.52746394e-01\n",
      "  2.38177758e-02  2.50647292e-02  5.87506257e-02  2.59194486e-02\n",
      " -4.33931574e-02 -2.84972321e-02 -5.21255806e-02  4.72321510e-02\n",
      " -1.49643412e-02  4.45475802e-02 -5.05417101e-02  5.62943742e-02\n",
      " -3.19287069e-02  2.91789696e-02 -7.96991810e-02 -6.06839247e-02\n",
      "  3.94682921e-02 -1.00327559e-01 -4.70448472e-03 -1.22331031e-01\n",
      " -1.55161768e-02  7.43733123e-02 -9.34893116e-02  9.85433906e-02\n",
      "  4.28674882e-03 -4.45325151e-02 -3.99576984e-02  7.63932914e-02\n",
      "  1.28972251e-02 -3.60970683e-02  8.02477300e-02  1.27256766e-01\n",
      " -2.39403211e-02 -4.69989367e-02  3.63007337e-02  1.18652377e-02\n",
      "  4.51930799e-02  6.88063502e-02  5.15041547e-03  2.57807914e-02\n",
      "  1.02394450e-04  6.75002262e-02  6.01080656e-02 -9.57538001e-03\n",
      "  5.93877770e-02  5.47390394e-02  1.32679701e-01 -4.86527197e-03\n",
      " -3.73045206e-02  4.35009561e-02  1.18667386e-01  1.32130012e-02\n",
      " -5.53853922e-02 -1.07975001e-03 -3.76958065e-02 -5.21113127e-02\n",
      "  6.06016442e-02  2.16881763e-02 -8.89433920e-02  4.39945003e-03\n",
      "  4.08999398e-02 -3.40677984e-02  2.87881959e-02 -1.18753472e-02\n",
      "  3.10503375e-02  6.47389665e-02  1.46759793e-01  4.78283735e-03\n",
      " -6.83225766e-02  7.45650604e-02 -1.11852340e-01 -4.30738069e-02\n",
      " -1.33047640e-01  8.75815153e-02 -1.08636655e-01 -6.22772053e-02\n",
      " -1.21547073e-01  5.85817620e-02 -3.75878550e-02  1.23675033e-01\n",
      "  8.17659646e-02  2.55654007e-02  7.91934878e-03 -8.85255635e-02\n",
      "  4.85880226e-02 -3.12523544e-02  7.11743459e-02  1.19776182e-01\n",
      " -8.22497830e-02  6.38516620e-02 -1.85670286e-01  5.54984510e-02\n",
      "  7.79012293e-02  7.51828775e-02  5.39621860e-02  1.41593525e-02\n",
      " -1.70663595e-02 -1.36548169e-02  4.58595753e-02  8.81101191e-02\n",
      "  4.34355922e-02  2.50473022e-02  1.13227390e-01 -3.85816656e-02\n",
      "  1.21502846e-01 -1.03183024e-01 -2.93460861e-02  6.35390915e-03\n",
      " -9.97771174e-02 -1.73141365e-03  4.75072078e-02 -6.09258888e-03\n",
      "  3.61997001e-02 -4.35612984e-02  1.43838450e-01  6.35560369e-03\n",
      "  5.14263613e-03  3.29888575e-02  5.32949120e-02 -5.16598932e-02\n",
      "  7.41354451e-02 -2.55965423e-02  6.57750815e-02 -2.21620947e-02\n",
      "  3.15903611e-02 -4.48293202e-02 -6.69704601e-02  2.75700577e-02\n",
      "  1.38729122e-02 -1.00098178e-01 -5.19680418e-02 -7.13002682e-02]\n"
     ]
    }
   ],
   "source": [
    "print(vectors_matrix[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vectors_matrix[10:5000]\n",
    "#print(X_train[1])\n",
    "Y_train=Yt[10:5000]\n",
    "#print(Y_train[10])\n",
    "X_test=vectors_matrix[5001:]\n",
    "Y_test=Yt[5001:]\n",
    "#print(X_test[10])\n",
    "#print(Y_test[5001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 100, 16)           2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 50, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                13400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                462       \n",
      "=================================================================\n",
      "Total params: 178,070\n",
      "Trainable params: 178,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4990 samples, validate on 2241 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Maya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "4990/4990 [==============================] - 7s 1ms/sample - loss: 2.5332 - acc: 0.1529 - val_loss: 2.6219 - val_acc: 0.1129\n",
      "Epoch 2/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4743 - acc: 0.1515 - val_loss: 2.5803 - val_acc: 0.1129\n",
      "Epoch 3/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4667 - acc: 0.1491 - val_loss: 2.6013 - val_acc: 0.1129\n",
      "Epoch 4/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4630 - acc: 0.1509 - val_loss: 2.5929 - val_acc: 0.1129\n",
      "Epoch 5/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4619 - acc: 0.1513 - val_loss: 2.5859 - val_acc: 0.1129\n",
      "Epoch 6/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4608 - acc: 0.1557 - val_loss: 2.5792 - val_acc: 0.1352\n",
      "Epoch 7/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4608 - acc: 0.1453 - val_loss: 2.6027 - val_acc: 0.1129\n",
      "Epoch 8/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4611 - acc: 0.1543 - val_loss: 2.5869 - val_acc: 0.1129\n",
      "Epoch 9/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4595 - acc: 0.1535 - val_loss: 2.5978 - val_acc: 0.1129\n",
      "Epoch 10/10\n",
      "4990/4990 [==============================] - 5s 1ms/sample - loss: 2.4588 - acc: 0.1549 - val_loss: 2.5907 - val_acc: 0.1352\n",
      "Accuracy: 13.52%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.python.framework\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.keras.backend\n",
    "import numpy\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "#X_train=train\n",
    "#Y_train=ltrain\n",
    "#X_test=test\n",
    "#Y_test=ltest\n",
    "num_classes = 14\n",
    "max_mahdar_length = 100\n",
    "#sequences=train\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_mahdar_length) #, truncatng)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_mahdar_length) #,dtype='int32')\n",
    "Y_train = to_categorical(Y_train, num_classes)# mn7awel l y la one hot unicoding yaani vecteur kl chi zero ma 3ada class tab3ha bi7eta 1\n",
    "Y_test  = keras.utils.to_categorical(Y_test, num_classes)\n",
    "top_words=5000\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_mahdar_length))\n",
    "model.add(Conv1D(filters=16, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(32,activation='relu'))# yaani 128 neurones\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy , optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.99526468\n",
      "Iteration 2, loss = 3.52027014\n",
      "Iteration 3, loss = 3.42364991\n",
      "Iteration 4, loss = 3.41318532\n",
      "Iteration 5, loss = 3.41181890\n",
      "Iteration 6, loss = 3.41246669\n",
      "Iteration 7, loss = 3.41228931\n",
      "Iteration 8, loss = 3.41160408\n",
      "Iteration 9, loss = 3.41341597\n",
      "Iteration 10, loss = 3.41594193\n",
      "Iteration 11, loss = 3.41302295\n",
      "Iteration 12, loss = 3.41518361\n",
      "Iteration 13, loss = 3.41164742\n",
      "Iteration 14, loss = 3.41271271\n",
      "Iteration 15, loss = 3.41397465\n",
      "Iteration 16, loss = 3.41232506\n",
      "Iteration 17, loss = 3.41056363\n",
      "Iteration 18, loss = 3.41301879\n",
      "Iteration 19, loss = 3.41226056\n",
      "Iteration 20, loss = 3.41210582\n",
      "Iteration 21, loss = 3.41297397\n",
      "Iteration 22, loss = 3.41147127\n",
      "Iteration 23, loss = 3.41459637\n",
      "Iteration 24, loss = 3.41107278\n",
      "Iteration 25, loss = 3.41027125\n",
      "Iteration 26, loss = 3.41326955\n",
      "Iteration 27, loss = 3.41339301\n",
      "Iteration 28, loss = 3.41226830\n",
      "Iteration 29, loss = 3.41359353\n",
      "Iteration 30, loss = 3.41111852\n",
      "Iteration 31, loss = 3.41172452\n",
      "Iteration 32, loss = 3.41350038\n",
      "Iteration 33, loss = 3.41390446\n",
      "Iteration 34, loss = 3.41269980\n",
      "Iteration 35, loss = 3.41389548\n",
      "Iteration 36, loss = 3.41357355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(50,30,), max_iter=150, solver='adam',verbose =10 , tol=1e-4, random_state=2, learning_rate_init=0.01)\n",
    "model=mlp.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.000000\n",
      "test Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"train Accuracy: %f\" %mlp.score(X_train,Y_train))\n",
    "print(\"test Accuracy: %f\" %mlp.score(X_test,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
